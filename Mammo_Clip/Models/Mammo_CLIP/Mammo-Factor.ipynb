{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30468f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nhat Minh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import json\n",
    "import abc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from DatModule.datamodule import DataModule, get_transforms, MammoDataset_Mapper\n",
    "from breastclip.model import BreastClip, MammoClassification, MammoEfficientNet, load_image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09e0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mammo_factor_model(model_config: Dict, loss_config: Dict, tokenizer: PreTrainedTokenizer = None) -> nn.Module:\n",
    "    name = model_config[\"name\"].lower()\n",
    "    if name == \"clip_custom\":\n",
    "        return BreastClip(model_config, loss_config, tokenizer)\n",
    "    elif name == \"finetune_classification\":\n",
    "        model_type = model_config[\"image_encoder\"].get(\"model_type\", \"vit\")\n",
    "        return MammoClassification(model_config, model_type)\n",
    "    elif name == \"pretrained_classifier\":\n",
    "        return MammoEfficientNet(model_config)\n",
    "    else:\n",
    "        raise KeyError(f\"Not supported model: {model_config['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36383743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapper_model(torch.nn.Module):\n",
    "    def __init__(self, ckpt, lang_emb: int, emb_dim: int, one_proj: bool, adapter: bool, attr_embs):\n",
    "        super(Mapper_model, self).__init__()\n",
    "        self.image_encoder = load_image_encoder(ckpt[\"config\"][\"model\"][\"image_encoder\"])\n",
    "        image_encoder_weights = {}\n",
    "        for k in ckpt[\"model\"].keys():\n",
    "            if k.startswith(\"image_encoder.\"):\n",
    "                image_encoder_weights[\".\".join(k.split(\".\")[1:])] = ckpt[\"model\"][k]\n",
    "        self.image_encoder.load_state_dict(image_encoder_weights, strict=True)\n",
    "        self.image_encoder_type = ckpt[\"config\"][\"model\"][\"image_encoder\"][\"model_type\"]\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.lang_emb = lang_emb\n",
    "        self.one_proj = one_proj\n",
    "        self.adapter = adapter\n",
    "\n",
    "        if self.one_proj:\n",
    "            self.num_proj = 1\n",
    "        else:\n",
    "            self.num_proj = len(attr_embs)\n",
    "        self.pool = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Linear(self.emb_dim, self.emb_dim),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(self.emb_dim, self.lang_emb),\n",
    "                )\n",
    "                for i in range(self.num_proj)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def encode_image(self, input):\n",
    "        image_features, raw_features = self.image_encoder(input)\n",
    "        return image_features, raw_features\n",
    "\n",
    "    def forward(self, sample: dict):\n",
    "        out_dict = {}\n",
    "\n",
    "        img_vector = sample[\"img\"].to(torch.float32).to(\"cuda\")\n",
    "        if len(img_vector.size()) == 5:\n",
    "            img_vector = img_vector.squeeze(1).permute(0, 3, 1, 2)\n",
    "        input = {\"image\": img_vector}\n",
    "\n",
    "        image_features, raw_features = self.encode_image(input)\n",
    "        bs = raw_features.size(0)\n",
    "        channel_dim = raw_features.size(1)\n",
    "        raw_features_flatten = raw_features.view(bs, channel_dim, -1)\n",
    "\n",
    "        out_img_a = []\n",
    "        for i in range(self.num_proj):\n",
    "            pool = self.pool[i](raw_features_flatten)\n",
    "            if self.adapter:\n",
    "                pool = 0.2 * pool + 0.8 * raw_features_flatten\n",
    "            out_img_a.append(pool)\n",
    "\n",
    "        region_proj_embs = torch.cat(out_img_a, dim=1).view(\n",
    "            -1, self.num_proj, self.lang_emb\n",
    "        )\n",
    "        out_dict[\"region_proj_embs\"] = region_proj_embs\n",
    "        out_dict[\"num_regions\"] = torch.tensor(channel_dim)\n",
    "        out_dict[\"image_features\"] = image_features\n",
    "        out_dict[\"raw_features\"] = raw_features\n",
    "        return out_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7081a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLoss(torch.nn.Module):\n",
    "    __metaclass__ = abc.ABC\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.iteration = 0\n",
    "        self.running_loss = 0\n",
    "        self.mean_running_loss = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "    def update_running_loss(self, loss):\n",
    "        self.iteration += 1\n",
    "        self.running_loss += loss.item()\n",
    "        self.mean_running_loss = self.running_loss / self.iteration\n",
    "\n",
    "\n",
    "class Mapper_loss(BaseLoss):\n",
    "    def __init__(self, temp: float, one_proj: bool, attr_to_embs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.temperature = temp\n",
    "        self.one_proj = one_proj\n",
    "\n",
    "        self.attr_embs = []\n",
    "        for a in attr_to_embs:\n",
    "            self.attr_embs.append(attr_to_embs[a])\n",
    "        self.attr_embs = torch.tensor(np.stack(self.attr_embs)).cuda().to(torch.float32)\n",
    "\n",
    "    def forward(self, pred: dict, sample: dict):\n",
    "        anchor_img = torch.nn.functional.normalize(\n",
    "            pred[\"region_proj_embs\"].float(), dim=2\n",
    "        )\n",
    "        labels = sample[\"labels\"].to(int)\n",
    "\n",
    "        attr_ids = labels.sum(0).nonzero().flatten().tolist()\n",
    "        batch_size = labels.shape[0]\n",
    "\n",
    "        loss = torch.tensor(0.0).cuda()\n",
    "        num_loss_terms = 0\n",
    "\n",
    "        if self.one_proj:\n",
    "            txt_emb = self.attr_embs[attr_ids, :].T.unsqueeze(0)\n",
    "            sim = (anchor_img @ txt_emb).squeeze() / self.temperature\n",
    "        else:\n",
    "            reg_emb = anchor_img[:, attr_ids, :]\n",
    "            txt_emb = self.attr_embs[attr_ids, :].unsqueeze(0)\n",
    "            sim = (reg_emb * txt_emb).sum(2) / self.temperature\n",
    "\n",
    "        split = torch.split(sim, pred[\"num_regions\"].tolist(), dim=0)\n",
    "        vals, _ = zip(*map(torch.max, split, [0] * batch_size))\n",
    "\n",
    "        sim = torch.stack(vals)  # batch_size x len(attr_ids)\n",
    "        true_label = labels[:, attr_ids].cuda()\n",
    "        inv_true_label = (~true_label.bool()).to(int)\n",
    "\n",
    "        # Compute final contrastive loss\n",
    "        denom = torch.exp(sim) + torch.exp(sim * inv_true_label).sum(0, keepdims=True)\n",
    "        loss = ((-torch.log(torch.exp(sim) / denom)) * true_label).sum(1, keepdims=True)\n",
    "        num_loss_terms = true_label.sum()\n",
    "        loss = loss.sum() / num_loss_terms\n",
    "        self.update_running_loss(loss)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fecd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRIBUTES = [\n",
    "    \"mass\",\n",
    "    \"suspicious_calcification\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903bf5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attribute_embs(out_dir, breast_clip_path, model_name):\n",
    "    def get_prompts(attr):\n",
    "        if attr == \"mass\":\n",
    "            return [\n",
    "                # your mass prompts here\n",
    "            ]\n",
    "        elif attr == \"suspicious_calcification\":\n",
    "            return [\n",
    "                # your calcification prompts here\n",
    "            ]\n",
    "        return []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    breast_clip_path = Path(breast_clip_path)\n",
    "    ckpt = torch.load(breast_clip_path, map_location=device)\n",
    "    cfg = ckpt[\"config\"]\n",
    "\n",
    "    datamodule = DataModule(\n",
    "        data_config=cfg[\"data_train\"],\n",
    "        dataloader_config=cfg[\"dataloader\"],\n",
    "        tokenizer_config=cfg.get(\"tokenizer\"),\n",
    "        loss_config=cfg[\"loss\"],\n",
    "        transform_config=cfg[\"transform\"],\n",
    "        mean=cfg[\"base\"][\"mean\"],\n",
    "        std=cfg[\"base\"][\"std\"],\n",
    "        image_encoder_type=cfg[\"model\"][\"image_encoder\"][\"model_type\"],\n",
    "        cur_fold=cfg[\"base\"][\"fold\"],\n",
    "    )\n",
    "\n",
    "    clip = mammo_factor_model(cfg[\"model\"], cfg[\"loss\"], datamodule.tokenizer)\n",
    "    clip = clip.to(device)\n",
    "    clip.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    clip.eval()\n",
    "\n",
    "    attr_embs = []\n",
    "    with torch.no_grad():\n",
    "        for attr in ATTRIBUTES:\n",
    "            prompts = get_prompts(attr)\n",
    "            tokens = datamodule.tokenizer(\n",
    "                prompts,\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=256,\n",
    "            ).to(device)\n",
    "            txt_feats = clip.encode_text(tokens)\n",
    "            if hasattr(clip, \"projection\") and clip.projection:\n",
    "                txt_feats = clip.text_projection(txt_feats)\n",
    "            avg_emb = txt_feats.mean(dim=0, keepdim=True)\n",
    "            avg_emb = avg_emb / avg_emb.norm(dim=-1, keepdim=True)\n",
    "            attr_embs.append(avg_emb)\n",
    "\n",
    "    attr_embs = torch.cat(attr_embs, dim=0).cpu().numpy()\n",
    "    attr_to_emb = dict(zip(ATTRIBUTES, attr_embs))\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_path = out_dir / f\"attr_embs_{model_name}.pth\"\n",
    "    torch.save(attr_to_emb, save_path)\n",
    "    print(f\"Saved {len(attr_to_emb)} attribute embeddings to {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65113c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapper_model(nn.Module):\n",
    "    def __init__(self, ckpt, lang_emb: int, emb_dim: int, one_proj: bool, adapter: bool, attr_embs):\n",
    "        super().__init__()\n",
    "        # load frozen image encoder\n",
    "        self.image_encoder = load_image_encoder(ckpt[\"config\"][\"model\"][\"image_encoder\"])\n",
    "        enc_w = {\n",
    "            \".\".join(k.split(\".\")[1:]): v\n",
    "            for k, v in ckpt[\"model\"].items()\n",
    "            if k.startswith(\"image_encoder.\")\n",
    "        }\n",
    "        self.image_encoder.load_state_dict(enc_w, strict=True)\n",
    "        for p in self.image_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.lang_emb = lang_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        self.one_proj = one_proj\n",
    "        self.adapter = adapter\n",
    "        self.num_proj = 1 if one_proj else len(attr_embs)\n",
    "\n",
    "        # projection heads\n",
    "        self.pool = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.emb_dim, self.emb_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.emb_dim, self.lang_emb),\n",
    "            )\n",
    "            for _ in range(self.num_proj)\n",
    "        ])\n",
    "\n",
    "    def encode_image(self, x):\n",
    "        return self.image_encoder(x)\n",
    "\n",
    "    def forward(self, sample: dict):\n",
    "        img = sample[\"img\"].to(torch.float32).to(next(self.parameters()).device)\n",
    "        if img.ndim == 5:\n",
    "            img = img.squeeze(1).permute(0, 3, 1, 2)\n",
    "\n",
    "        image_feats, raw = self.encode_image({\"image\": img})\n",
    "        bs, c, h, w = raw.shape\n",
    "        regions = raw.view(bs, c, -1)  # (B, C, N)\n",
    "\n",
    "        proj_outs = []\n",
    "        for i in range(self.num_proj):\n",
    "            p = self.pool[i](regions)  # (B, C, lang_emb)\n",
    "            if self.adapter:\n",
    "                p = 0.2 * p + 0.8 * regions\n",
    "            proj_outs.append(p)\n",
    "\n",
    "        region_proj_embs = torch.stack(proj_outs, dim=1)  # (B, num_proj, lang_emb)\n",
    "        return {\n",
    "            \"region_proj_embs\": region_proj_embs,\n",
    "            \"num_regions\": torch.tensor(c),\n",
    "            \"image_features\": image_feats,\n",
    "            \"raw_features\": raw,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dddcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapper_loss(nn.Module):\n",
    "    def __init__(self, temp: float, one_proj: bool, attr_to_embs):\n",
    "        super().__init__()\n",
    "        self.temperature = temp\n",
    "        self.one_proj = one_proj\n",
    "        embs = [attr_to_embs[a] for a in ATTRIBUTES]\n",
    "        self.attr_embs = torch.tensor(np.stack(embs), dtype=torch.float32).cuda()\n",
    "\n",
    "    def forward(self, pred: dict, sample: dict):\n",
    "        anchor = nn.functional.normalize(pred[\"region_proj_embs\"], dim=2)\n",
    "        labels = sample[\"labels\"].to(torch.int)\n",
    "        attr_ids = labels.sum(0).nonzero().flatten().tolist()\n",
    "        batch = labels.size(0)\n",
    "\n",
    "        if self.one_proj:\n",
    "            txt = self.attr_embs[attr_ids].T.unsqueeze(0)\n",
    "            sim = (anchor @ txt).squeeze() / self.temperature\n",
    "        else:\n",
    "            reg = anchor[:, attr_ids, :]\n",
    "            txt = self.attr_embs[attr_ids].unsqueeze(0)\n",
    "            sim = (reg * txt).sum(2) / self.temperature\n",
    "\n",
    "        # max over regions\n",
    "        sims = []\n",
    "        for i in range(batch):\n",
    "            sims.append(sim[i].max(0)[0])\n",
    "        sim = torch.stack(sims)\n",
    "\n",
    "        true = labels[:, attr_ids].cuda()\n",
    "        inv_true = (~true.bool()).int()\n",
    "        denom = torch.exp(sim) + torch.exp(sim * inv_true).sum(0, keepdim=True)\n",
    "        loss = (-torch.log(torch.exp(sim) / denom) * true).sum() / true.sum()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d346de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(args):\n",
    "    df = pd.read_csv(Path(args.data_dir) / args.csv_file).fillna(0)\n",
    "    df = df[(df[\"Mass\"] == 1) | (df[\"Suspicious_Calcification\"] == 1)]\n",
    "    train_df = df[df[\"split\"] == \"training\"].reset_index(drop=True)\n",
    "    valid_df = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "    train_ds = MammoDataset_Mapper(args, train_df, transform=get_transforms(args))\n",
    "    valid_ds = MammoDataset_Mapper(args, valid_df, transform=None)\n",
    "\n",
    "    if args.balanced_dataloader.lower() == \"y\":\n",
    "        w_path = Path(args.output_path) / f\"weights_fold{args.cur_fold}.pkl\"\n",
    "        if w_path.exists():\n",
    "            weights = pickle.load(open(w_path, \"rb\"))\n",
    "        else:\n",
    "            pos_w = args.sampler_weights[f\"fold{args.cur_fold}\"][\"pos_wt\"]\n",
    "            neg_w = args.sampler_weights[f\"fold{args.cur_fold}\"][\"neg_wt\"]\n",
    "            train_df[\"w\"] = train_df[\"cancer\"].map({1: pos_w, 0: neg_w})\n",
    "            weights = train_df[\"w\"].values\n",
    "            pickle.dump(weights, open(w_path, \"wb\"))\n",
    "\n",
    "        sampler = WeightedRandomSampler(weights.tolist(), len(weights), replacement=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=args.batch_size, sampler=sampler,\n",
    "                                  num_workers=args.num_workers, pin_memory=True, drop_last=True)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,\n",
    "                                  num_workers=args.num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=args.batch_size, shuffle=False,\n",
    "                              num_workers=args.num_workers, pin_memory=True)\n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7473de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, loss_fn, opt, train_loader, valid_loader, epochs, chk_pt_path, device):\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_val = float(\"inf\")\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        for sample in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
    "            opt.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(sample)\n",
    "                loss = loss_fn(pred, sample)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "\n",
    "        print(f\"Train epoch {epoch} done in {int(time.time() - t0)}s\")\n",
    "        torch.save(model.state_dict(), Path(chk_pt_path) / f\"epoch_{epoch}.pth\")\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sample in tqdm(valid_loader, desc=\"Validating\"):\n",
    "                pred = model(sample)\n",
    "                val_loss += loss_fn(pred, sample).item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), Path(chk_pt_path) / \"best.pth\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= 5:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69824c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu, torch version: 2.6.0+cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Mammo_CLIP/checkpoints/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m device    \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, torch version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m ckpt      \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_chk_pt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m attr_embs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(attr_embs_path)\n\u001b[0;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m Mapper_model(ckpt, lang_emb\u001b[38;5;241m=\u001b[39mlang_emb, emb_dim\u001b[38;5;241m=\u001b[39mimg_emb, one_proj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, adapter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, attr_embs\u001b[38;5;241m=\u001b[39mattr_embs)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Nhat Minh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Nhat Minh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Nhat Minh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Mammo_CLIP/checkpoints/'"
     ]
    }
   ],
   "source": [
    "data_dir           = \"/path/to/data\"\n",
    "csv_file           = \"./dataset/metadata.csv\"\n",
    "output_dir         = \"Outputs\"\n",
    "clip_chk_pt_path   = \"\"\n",
    "attr_embs_path     = \"\"\n",
    "\n",
    "lang_emb           = 512\n",
    "img_emb            = 1024\n",
    "batch_size         = 16\n",
    "num_workers        = 4\n",
    "lr                 = 5e-5\n",
    "epochs             = 20\n",
    "\n",
    "balanced_dataloader = False  # Set to True to use WeightedRandomSampler\n",
    "sampler_weights     = None\n",
    "cur_fold            = 0\n",
    "\n",
    "if balanced_dataloader:\n",
    "    with open(\"[Data Here]\", \"r\") as f:\n",
    "        sampler_weights = json.load(f)\n",
    "\n",
    "def get_Paths(output_dir):\n",
    "    base        = Path(output_dir)\n",
    "    chkpt_dir   = base / \"checkpoints\"\n",
    "    results_dir = base / \"results\"\n",
    "    tb_logs_dir = base / \"tb_logs\"\n",
    "    for d in (chkpt_dir, results_dir, tb_logs_dir):\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    return str(chkpt_dir), str(results_dir), str(tb_logs_dir)\n",
    "\n",
    "chkpt_path, results_path, tb_logs_path = get_Paths(output_dir)\n",
    "\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}, torch version: {torch.__version__}\")\n",
    "\n",
    "ckpt      = torch.load(clip_chk_pt_path, map_location=\"cpu\")\n",
    "attr_embs = torch.load(attr_embs_path)\n",
    "\n",
    "\n",
    "model = Mapper_model(ckpt, lang_emb=lang_emb, emb_dim=img_emb, one_proj=False, adapter=False, attr_embs=attr_embs).to(device)\n",
    "\n",
    "loss_fn = Mapper_loss(temp=0.07, one_proj=False, attr_to_embs=attr_embs).to(device)\n",
    "\n",
    "\n",
    "class Args: pass\n",
    "args = Args()\n",
    "args.data_dir            = data_dir\n",
    "args.csv_file            = csv_file\n",
    "args.batch_size          = batch_size\n",
    "args.num_workers         = num_workers\n",
    "args.balanced_dataloader = \"y\" if balanced_dataloader else \"n\"\n",
    "args.sampler_weights     = sampler_weights\n",
    "args.cur_fold            = cur_fold\n",
    "args.output_path         = results_path\n",
    "\n",
    "train_loader, valid_loader = get_dataloaders(args)\n",
    "\n",
    "train_loop(\n",
    "    model,\n",
    "    loss_fn,\n",
    "    opt,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    epochs=epochs,\n",
    "    chk_pt_path=chkpt_path,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558298f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
